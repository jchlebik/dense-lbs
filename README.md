# Density Aware LBS

This repo contains experimental approach of adding density into already existing [Learned Born Series solver](https://github.com/ucl-bug/lbs).

Python VENV used to run everything in this project is exported in the [requirements](requirements_tensorflow.txt) file.

To run training, a dataset is required. One used for experimenting can be generated using provided [config file](config/dataset_gen/dataset_generation_config.py) and a [generator](dataset_generator/dataset_generator.py)

e.g. of how to generate a dataset:
```
python dataset_generator/dataset_generator.py --config config/dataset_gen/dataset_generation_config.py --workdir <a path where to store the dataset>
```

To add density, many approaches were considered. Two showed to be much more successful than others:

1) Density added directly into the LBS, appending it to speed of sound parameter and retraining. This solution is called DLBS.
2) Train a corrector CNN, taking a result without accounting for density and correcting it to a correct, density aware ground truth.


To run training of the dlbs:
```
python main.py --config config/dlbs/24_stages_train_config.py.py --workdir <a path to store the resulting tensorboard file with training progress> --dataset_file_path <a path to dataset generated by the dataset_generator>
```

To run training of the corrector:
```
python main.py --config config/corrector/corrector_train_config.py --workdir <a path to store the resulting tensorboard file with training progress> --dataset_file_path <a path to dataset generated by the dataset_generator>
```

All testing done on karolina (8x A100) and barbora (4x V100) it4i clusters or our local group cluster (4xA5000)

# Misc other changes from LBS:

Improving the performance by parallelazing the training on multi-gpu systems by leveraging JAX's PMAP transformation (trainers/default_trainer.py)

Input pipeline reworked completely to use tensorflow instead of pytorch. Google search and even my own internal testing showed the tf input pipeline to be much more performant with better configuration options.

Training progress logging done through tensorboard instead of wandb due to the cluster limitations on opened ports. 

Model configuration done through ml_collections config files.

Strategy design pattern employed at multiple places to allow for easy extending of the solution with new models. loggers, input pipelines, trainers or other.




